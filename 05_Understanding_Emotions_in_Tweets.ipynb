{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Understanding Emotions in Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMX9Z3qo31KexEuxMEXUQnl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raj-vijay/nl/blob/master/05_Understanding_Emotions_in_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbYt1LamrKp6"
      },
      "source": [
        "**Emotion\tDetection\tin\tText**\n",
        "\n",
        "We\troutinely\texperience\temotions\tsuch\tas\thappiness,\tanger,\tsadness\tetc.\tAs\thumans,\ton\treading\t \"Why\tdon't\tyou\tever\t text\tme!\",\twe\tcan\teither\tinterpret\tit\tas\ta\tsad\tor\tan\tangry\temotion\tin\tabsence\tof\tcontext;\tand\tthe\tsame\tambiguity\texists\tfor\tmachines\tas\twell.\tLack\tof\tfacial\texpressions\tand\tvoice\tmodulations\tmake\tdetecting\temotions\tin\ttext\ta\tchallenging\tproblem.\t\n",
        "\n",
        "However,\tas\twe\tincreasingly\tcommunicate\tusing\t text\tmessaging\tapplications and\tdigital\tagents,\tcontextual\temotion\tdetection\tin\ttext\tis\tgaining\timportance\tto\tprovide\temotionally\taware\tresponses to\tusers.\t\n",
        "\n",
        "The objective of this Lab is to apply Lexicon and Machine Learning methods to improve the classification results of emotions in Tweets.\n",
        "\n",
        "In\t this\t task,\t you\t are\t given\t a\t dataset\t (NLP_Lab6_text_emotion.csv),\tavailable\ton\tCanvas,\tof 40,000\ttweets\tin\ttotal,\tlabelled\tinto\t13 different\thuman\tsentiments,\tyou\thave\tto\tclassify\tthe\temotion\tof\tuser\tas\tone\tof\tthe\temotion\tclasses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWyhk-1fr0Le"
      },
      "source": [
        "**Data Set Format**\n",
        "\n",
        "The Training dataset is a .csv file containing 4 columns:\n",
        "\n",
        "1. ID - Contains a unique number to identify each training sample\n",
        "2. Sentiment - Contains the human judged label of Emotion\n",
        "3. author - Contains the Twitter user ID (we will not use it in this task)\n",
        "4. Content - Contains the tweet\n",
        "\n",
        "Examples of\ttraining\tsamples\tare\tgiven\tbelow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouRr31-woLY8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word\n",
        "import re\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S7mbqeFblai"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcXtKDq9sHr8",
        "outputId": "4cf181d4-69d3-48d9-ecfb-f86e1ac21c24"
      },
      "source": [
        "!wget https://github.com/raj-vijay/nl/raw/master/files/text_emotion.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-16 11:59:40--  https://github.com/raj-vijay/nl/raw/master/files/text_emotion.csv\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/raj-vijay/nl/master/files/text_emotion.csv [following]\n",
            "--2021-06-16 11:59:40--  https://raw.githubusercontent.com/raj-vijay/nl/master/files/text_emotion.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4213490 (4.0M) [text/plain]\n",
            "Saving to: ‘text_emotion.csv’\n",
            "\n",
            "text_emotion.csv    100%[===================>]   4.02M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-06-16 11:59:41 (39.0 MB/s) - ‘text_emotion.csv’ saved [4213490/4213490]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxfk4uANbpXF"
      },
      "source": [
        "data = pd.read_csv('text_emotion.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "qYGmGtaBcPWJ",
        "outputId": "2d98c77f-9512-4689-e092-d50cba2a4356"
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>author</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1956967341</td>\n",
              "      <td>empty</td>\n",
              "      <td>xoshayzers</td>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1956967666</td>\n",
              "      <td>sadness</td>\n",
              "      <td>wannamama</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1956967696</td>\n",
              "      <td>sadness</td>\n",
              "      <td>coolfunky</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1956967789</td>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>czareaquino</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1956968416</td>\n",
              "      <td>neutral</td>\n",
              "      <td>xkilljoyx</td>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1956968477</td>\n",
              "      <td>worry</td>\n",
              "      <td>xxxPEACHESxxx</td>\n",
              "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1956968487</td>\n",
              "      <td>sadness</td>\n",
              "      <td>ShansBee</td>\n",
              "      <td>I should be sleep, but im not! thinking about ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1956968636</td>\n",
              "      <td>worry</td>\n",
              "      <td>mcsleazy</td>\n",
              "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1956969035</td>\n",
              "      <td>sadness</td>\n",
              "      <td>nic0lepaula</td>\n",
              "      <td>@charviray Charlene my love. I miss you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1956969172</td>\n",
              "      <td>sadness</td>\n",
              "      <td>Ingenue_Em</td>\n",
              "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     tweet_id  ...                                            content\n",
              "0  1956967341  ...  @tiffanylue i know  i was listenin to bad habi...\n",
              "1  1956967666  ...  Layin n bed with a headache  ughhhh...waitin o...\n",
              "2  1956967696  ...                Funeral ceremony...gloomy friday...\n",
              "3  1956967789  ...               wants to hang out with friends SOON!\n",
              "4  1956968416  ...  @dannycastillo We want to trade with someone w...\n",
              "5  1956968477  ...  Re-pinging @ghostridah14: why didn't you go to...\n",
              "6  1956968487  ...  I should be sleep, but im not! thinking about ...\n",
              "7  1956968636  ...               Hmmm. http://www.djhero.com/ is down\n",
              "8  1956969035  ...            @charviray Charlene my love. I miss you\n",
              "9  1956969172  ...         @kelcouch I'm sorry  at least it's Friday?\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyJaE7R8cXpA"
      },
      "source": [
        "data = data.drop('author', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "JT0f00jNcama",
        "outputId": "0d752129-2b9e-452b-bdf0-59bfedbc8536"
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1956967341</td>\n",
              "      <td>empty</td>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1956967666</td>\n",
              "      <td>sadness</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1956967696</td>\n",
              "      <td>sadness</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1956967789</td>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1956968416</td>\n",
              "      <td>neutral</td>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1956968477</td>\n",
              "      <td>worry</td>\n",
              "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1956968487</td>\n",
              "      <td>sadness</td>\n",
              "      <td>I should be sleep, but im not! thinking about ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1956968636</td>\n",
              "      <td>worry</td>\n",
              "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1956969035</td>\n",
              "      <td>sadness</td>\n",
              "      <td>@charviray Charlene my love. I miss you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1956969172</td>\n",
              "      <td>sadness</td>\n",
              "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     tweet_id   sentiment                                            content\n",
              "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
              "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
              "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
              "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
              "4  1956968416     neutral  @dannycastillo We want to trade with someone w...\n",
              "5  1956968477       worry  Re-pinging @ghostridah14: why didn't you go to...\n",
              "6  1956968487     sadness  I should be sleep, but im not! thinking about ...\n",
              "7  1956968636       worry               Hmmm. http://www.djhero.com/ is down\n",
              "8  1956969035     sadness            @charviray Charlene my love. I miss you\n",
              "9  1956969172     sadness         @kelcouch I'm sorry  at least it's Friday?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH2jUrPDcdI1"
      },
      "source": [
        " # Making all letters lowercase\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePg2rIpwch12"
      },
      "source": [
        " # Removing Punctuation, Symbols\n",
        "data['content'] = data['content'].str.replace('[^\\w\\s]',' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_swTTut7ckow",
        "outputId": "feb97c02-baf6-490d-f557-be72e92022a8"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNoyCO5Pc8QD",
        "outputId": "1ad72f52-8214-4f75-c251-87024ab8512c"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4yHBu95cncp"
      },
      "source": [
        "# Removing Stop Words using NLTK\n",
        "stop = stopwords.words('english')\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMb1kjiYcq7r"
      },
      "source": [
        "#Lemmatisation\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrQ5JtMcdBbS"
      },
      "source": [
        "#Correcting Letter Repetitions\n",
        "def de_repeat(text):\n",
        " pattern = re.compile(r\"(.)\\1{2,}\")\n",
        " return pattern.sub(r\"\\1\\1\", text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My8q_oDvdG4k"
      },
      "source": [
        "data['content'] = data['content'].apply(lambda x: \" \".join(de_repeat(x) for x in x.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoW_jPiDdKJo"
      },
      "source": [
        " # Code to find the top 10,000 rarest words appearing in the data\n",
        "freq = pd.Series(' '.join(data['content']).split()).value_counts()[-10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEVAz666dNaS"
      },
      "source": [
        "# Removing all those rarely appearing words from the data\n",
        "freq = list(freq.index)\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoosEwApdRva"
      },
      "source": [
        " #Encoding output labels\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y = lbl_enc.fit_transform(data.sentiment.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6rQeuPedqAX"
      },
      "source": [
        " # Splitting into training and testing data in 80:20 ratio\n",
        "X_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=\n",
        "0.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSBLtzwLdxD8"
      },
      "source": [
        "# Extracting TF-IDF parameters\n",
        "tfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf.fit_transform(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8utwp4Qd0-N"
      },
      "source": [
        "# Extracting Count Vectors Parameters\n",
        "count_vect = CountVectorizer(analyzer='word')\n",
        "count_vect.fit(data['content'])\n",
        "X_train_count = count_vect.transform(X_train)\n",
        "X_val_count = count_vect.transform(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtghKBmgd4OZ",
        "outputId": "e7fc4986-52c9-424e-ba8c-cfb9af8799e7"
      },
      "source": [
        "# Model 1: Multinomial Naive Bayes Classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_tfidf, y_train)\n",
        "y_pred = nb.predict(X_val_tfidf)\n",
        "print('naive bayes tfidf accuracy %s' % accuracy_score(y_pred, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "naive bayes tfidf accuracy 0.24475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhSDCXKwd9js",
        "outputId": "fe5a5009-f114-4f08-c1e0-1b8df8ed819d"
      },
      "source": [
        "# Model 2: Linear SVM\n",
        "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
        "lsvm.fit(X_train_tfidf, y_train)\n",
        "y_pred = lsvm.predict(X_val_tfidf)\n",
        "print('svm using tfidf accuracy %s' % accuracy_score(y_pred, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "svm using tfidf accuracy 0.209375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWvYI-xteBus",
        "outputId": "a9576815-83be-443b-9911-5d0d134cffef"
      },
      "source": [
        "# Model 3: logistic regression\n",
        "logreg = LogisticRegression(C=1)\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "y_pred = logreg.predict(X_val_tfidf)\n",
        "print('log reg tfidf accuracy %s' % accuracy_score(y_pred, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log reg tfidf accuracy 0.251375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E36mTkK8eGz_",
        "outputId": "511028c4-e7e2-433d-ec78-0628d06b4b83"
      },
      "source": [
        "# Model 4: Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=500)\n",
        "rf.fit(X_train_tfidf, y_train)\n",
        "y_pred = rf.predict(X_val_tfidf)\n",
        "print('random forest tfidf accuracy %s' % accuracy_score(y_pred, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random forest tfidf accuracy 0.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pploLl-0fcPs",
        "outputId": "f892b023-4777-4b27-d273-b2e2c5cb9581"
      },
      "source": [
        "## Building models using count vectors feature\n",
        "# Model 1: Multinomial Naive Bayes Classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_count, y_train)\n",
        "y_pred = nb.predict(X_val_count)\n",
        "print('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "naive bayes count vectors accuracy 0.324625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgEZdADSfehc",
        "outputId": "6e5537cc-b42e-4179-d76b-14a29d981fee"
      },
      "source": [
        "# Model 2: Linear SVM\n",
        "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
        "lsvm.fit(X_train_count, y_train)\n",
        "y_pred = lsvm.predict(X_val_count)\n",
        "print('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lsvm using count vectors accuracy 0.329375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpChNROOfjRu",
        "outputId": "e56ce9e6-16d2-4920-cd48-c0791e54dd3b"
      },
      "source": [
        "# Model 3: Logistic Regression\n",
        "logreg = LogisticRegression(C=1)\n",
        "logreg.fit(X_train_count, y_train)\n",
        "y_pred = logreg.predict(X_val_count)\n",
        "print('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log reg count vectors accuracy 0.334625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7WAg-M5fqQ7",
        "outputId": "30ab6bbe-57b3-4613-b01d-0b3d7c5bae25"
      },
      "source": [
        "# Model 4: Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=500)\n",
        "rf.fit(X_train_count, y_train)\n",
        "y_pred = rf.predict(X_val_count)\n",
        "print('random forest with count vectors accuracy %s' % accuracy_score(y_pred,y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random forest with count vectors accuracy 0.319125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg4HZYcimeBx"
      },
      "source": [
        "#Below are 8 random statements. The first 4 depict happiness. The last 4 depict sadness\n",
        "tweets = pd.DataFrame(['I am very happy today! The atmosphere looks cheerful',\n",
        "'Things are looking great. It was such a good day',\n",
        "'Success is right around the corner. Lets celebrate this victory',\n",
        "'Everything is more beautiful when you experience them with a smile!',\n",
        "'Now this is my worst, okay? But I am gonna get better.',\n",
        "'I am tired, boss. Tired of being on the road, lonely as a sparrow in the rain. I am tired of all the pain I feel',\n",
        "'This is quite depressing. I am filled with sorrow',\n",
        "'His death broke my heart. It was a sad day'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpO1a1kgmwAK"
      },
      "source": [
        "# Doing some preprocessing on these tweets as done before\n",
        "tweets[0] = tweets[0].str.replace('[^\\w\\s]',' ')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "tweets[0] = tweets[0].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "from textblob import Word\n",
        "tweets[0] = tweets[0].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iQfTCk-m4hV"
      },
      "source": [
        "# Extracting Count Vectors feature from our tweets\n",
        "tweet_count = count_vect.transform(tweets[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKzXyGnfm7Be",
        "outputId": "ca65644a-d89b-4b67-d8cb-50d05f9a1020"
      },
      "source": [
        "#Predicting the emotion of the tweet using our already trained linear SVM\n",
        "tweet_pred = lsvm.predict(tweet_count)\n",
        "print(tweet_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 5  5 12  5 12 12 10 10]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}